导读：昨天的清华大学大礼堂迎来了Facebook人工智能研究院院长Yann LeCun，就《深度学习与人工智能的未来》，他为现场百余听众展开了一场两个小时的讲座。

Yann LeCun在清华做了题为【深度学习与人工智能的未来】的演讲

Yann LeCun来清华讲座的消息早在半个月前就传遍了科技圈，一票难求。

本次演讲为清华经管学院、清华x-lab、Facebook合作开设课程《创新与创业：硅谷洞察》的第一讲，既是课程又是讲座，因此整个讲座风格也是颇学院派：从最早追溯到1957年神经科学的监督学习谈起，并对神经网络的训练做了一个基本的介绍。

演讲还讲述了近几年在计算机视觉（Computer Vision）、深度学习（Deep Learning）等方面的最新进展以及颇有未来潜力的技术方向--生成对抗学习（Adversarial Learning），娓娓道来且让人意犹未尽。

总的来说，LeCun的演讲在近期在几个大会上的演讲思路大致相同。虽然还没有完整的视频，但好奇的各位读者可以后台回复【facebook】获取一份近期LeCun的演讲ppt完整版及相关视频链接。

当然不远万里来到北京，LeCun还是给出了一些新的观点：针对同学们的提问，他谈及了中美AI研究的对比、最近的AI围棋大战，以及他最看好的AI发展方向。他认为交通、无人驾驶车、医疗、智能助理最有可能得到突破并改变所有人的生活。

在讲座的一开始有一个小插曲，Yann LeCun在华人AI圈一直被亲切的称为“杨乐康”，不知道这个称呼是不是被传到了他的耳朵里，干脆大张旗鼓的给自己取了个中文名字，在这次演讲开始前公布：【杨立昆】。

说人家叫杨音比的同学你出来

以下是大数据文摘从现场发回了本次讲座的最新内容速递。

Lecun现任Facebook AI研究院的院长，同时他也是美国纽约大学的终身教授。他的研究兴趣包括机器学习、计算机视觉、移动机器人以及计算神经学等。他因著名且影响深远的卷积神经网络（CNN）相关的工作而被人称为CNN之父。

Lecun演讲内容跨度数十载，从最早追溯到1957年神经科学的监督学习谈起并对神经网络的训练做了一个基本的介绍。接着Lecun重点讲解了他的成名作——卷积神经网络（CNN），还给大家展示录制于1993年的珍贵视频——年轻的Lecun在一台486 PC上编写的光学字符识别系统。

不知那时候还在攻读博士的Lecun是否想到他的研究成果在随后的二十年给整个世界带来了巨大的影响和翻天覆地的变化。

在当时的环境下，并不是所有的人都相信神经网络技术，更多的人倾向于一些有理论保障的机器学习方法，比如kernel machine等。

Lecun给大家展示了他在1995年所见证的两位机器学习前辈Jackel和Vapnik（当时他们都在贝尔实验室，Jackel是Vapnik的上司）的两个有趣赌局：第一个赌局中，Jackel声称最迟到2000年我们就会有一个关于大的神经网络为什么有效的理论解释，当然随后的历史证明他输了；第二个赌局中，Vapnik声称最迟到2000年没有人将会继续使用1995年的这些神经网络结构（意思是大家都会转而使用支持向量机SVM，Vapnik是SVM的发明人之一），结果Vapnik也输了。

不仅在2000年大家依然在用，直到今天，在结合了大数据与强大计算能力后，这些古老的神经网络结构迸发出更加巨大的能量。这个深度学习史上有趣的八卦，我们如今听来却也不胜唏嘘。技术的发展往往是螺旋式且兼具跳跃性，实在难以预料。正如今天的我们在清华的大礼堂里与Lecun一起畅想着深度学习与人工智能的未来，却不知十年、二十年后我们又在哪里用什么样的视角来看待今天的自己。

机器没有常识这一点不容置疑，而常识确实进行“预测”必不可少的一项技能，为了让机器拥有这一能力，Facebook的研究人员正试着将这一技术带入下一个阶段——从纯数据处理逻辑迈向堪与人类媲美的某种形式的「常识」。

机器要掌握常识需要搞清楚世界（物理世界、电子世界）的运作方式并作出合理决定，那么，它们必须能够获取大量的背景知识、了解世界的运行规律，进而作出准确的预测和计划。

LeCun 认为，人类需要和机器以一种非常自然的方式互动，因此，我们需要让机器懂得人类，也即掌握常识。要掌握这类常识，机器需要一个内在模型，告诉它世界的运行方式，这就要求机器具备预测能力。现在我们所欠缺的就是：无需人类加以训练，机器自己可以建构起这样一个内在模型。

人类的学习过程，就是对现实世界进行观察和体验。或许在不久未来，会出现一部彻底（complete）的人工智能系统，不仅能识别文本和图像，还能进行更高级别的推理，预测，规划等，思考和行为方式可与人类相媲美。

尽管未来是如此的难以预料，但科研的道路却是一步一个脚印的走出来的。Lecun接着给大家展示了一系列的技术干货，包括近几年在计算机视觉（Computer Vision）、深度学习（Deep Learning）等方面的最新进展以及颇有未来潜力的技术方向对抗学习（Adversarial Learning）。LeCun本身就是生成对抗学习的坚定支持者，也对于这方面的研究做出来重要贡献。

用 Ian Goodfellow 的话来说：“生成对抗网络是一种生成模型（Generative Model），其背后基本思想是从训练库里获取很多训练样本，从而学习这些训练案例生成的概率分布。

而实现的方法，是让两个网络相互竞争，‘玩一个游戏’。其中一个叫做生成器网络（ Generator Network），它不断捕捉训练库里真实图片的概率分布，将输入的随机噪声（Random Noise）转变成新的样本（也就是假数据）。另一个叫做判别器网络（Discriminator Network），它可以同时观察真实和假造的数据，判断这个数据到底是不是真的。”

对抗网络（adversarial networks ）也因此是一个训练机器预测能力新方法。一个对抗网络带有一个发生器（generator），从随机输入中生成某类数据（比如，图片）。还带有一个判别器（discriminator），它从发生器中获取输入，或者从一个真实数据组中获取输入，判别器必须区分来源不同的输入——判别真伪。两个神经网络能实现自身优化，从而生成更加真实的输入，以及世界观更为合理的网络。

为了展现这一点，LeCun和他的团队用各种图片数据组训练了DCGAN，这些图片采集了ImageNet数据组中一组特定图像，比如所展示的卧室。

DCGAN 也能够识别模式并将某些相似表征放在一起。比如，在脸部图像数据集中，生成器不理解什么是微笑的意义，但是，它能发现人类微笑图片的相似性，并将它们分为一组。

一旦我们可以训练一台机器去预测世界看起来什么样，那么，我们就能将机器学到的内容新的任务，比如视频预测。

当机器掌握了这个常识后，就会更善于找到最佳办法去完成某件特定任务最终，这类知识会进一步加快应用研发，包括看先进的聊天机器人和虚拟助手。

演讲结束后，清华的同学还就LeCun的演讲提出了几个问题：

A ：我认为有可能，比如，可以让计算机看一幅画，计算机将识别出画中的元素，并有再创造的可能。一个值得考虑的问题是时间，我们为什么要让计算机花时间去做这件事。

Q ：继谷歌AlphaGo之后，腾讯的产品FineArt（绝艺）也击败了人类围棋选手。您怎样看待东西方在人工智能方向的竞争？

A ：我觉得这非常令人激动。科研工作在变得更加国际化、彼此联系更加紧密。随着更多的免费论文和开源项目的出现，一个产品出现后，竞品往往会在世界各地迅速出现。但这也使我们能为某个问题找到更多解决方案。人工智能发展如此迅速的原因之一，就是来自不同国家的竞争促成的全球人工智能社区进步。

Q 人工智能领域，有哪些您觉得将会有重大突破的方向？

A ：首先，我认为是交通、无人驾驶车。这不仅将改变人们的出行方式，还将改变整个城市的设计和规划。对于中国这一汽车制造大国来说，这可能是一个机会。

另一个方面，是医疗和健康领域。个性化、定制化的药物、工具和治疗方式，将改变所有人的生活。最后一方面，是定制化的智能助理，将改变我们的工作方式。

Q ：您眼中的人工智能发展后的终极世界是什么样？

A ：我理想中的世界是一个“强人工智能”世界。我认为人工智能将促进改革和进步。我们没有必要惧怕人工智能。